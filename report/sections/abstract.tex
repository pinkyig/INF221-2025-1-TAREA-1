Este trabajo presenta un análisis experimental comparativo entre algoritmos de ordenamiento (SelectionSort, MergeSort y QuickSort) y de multiplicación de matrices (Naive y Strassen), evaluando la brecha entre su complejidad teórica y su rendimiento práctico. Implementados en C++, los algoritmos fueron sometidos a pruebas con conjuntos de datos de tamaños crecientes (hasta \(n = 10^7\) para ordenamiento y matrices de \(1024 \times 1024\)) y características variadas (aleatorios, ordenados, densos, diagonales y dispersos). Los resultados confirman que SelectionSort se vuelve inviable para \(n \geq 10^5\), mientras que los algoritmos de complejidad \(O(n \log n)\) mantienen eficiencia incluso con grandes volúmenes de datos, siendo \texttt{std::sort} el más rápido. Para la multiplicación de matrices, Strassen ofrece ventajas marginales solo en casos específicos (\(n \approx 256\)), pues la sobrecarga de recursión y gestión de memoria contrarresta sus beneficios teóricos en dimensiones mayores. Se concluye que la elección óptima de algoritmo depende no solo de su complejidad asintótica, sino también de las características del hardware y los patrones de datos, sugiriéndose la exploración de implementaciones híbridas para un alto rendimiento.